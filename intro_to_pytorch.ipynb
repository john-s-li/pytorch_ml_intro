{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to PyTorch and Neural Network Training\n",
    "#### *Author* : Johnathon Li (johnathon.li@momenta.ai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print(f\"Using torch version {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "\n",
    "pl = platform.system().lower()\n",
    "if \"darwin\" in pl:\n",
    "    # M1 GPU acceleration not stable as of yet so proceed at your own risk\n",
    "    device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for setting the seed\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors in PyTorch\n",
    "\n",
    "Tensors are generalizations of scalars, vectors, matrices and so on. You can see a scalar as a 0-th dimensional tensor, a vector 1D, a matrix 2D, etc. Tensors are similar to NumPy arrays, except they are **optimized for automatic differentiation and can run on GPUs**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tensor\n",
    "np.set_printoptions(precision=3)\n",
    "a = [1, 2, 3]\n",
    "b = np.array([4, 5, 6], dtype=int)\n",
    "t_a = torch.tensor(a) # make a tensor\n",
    "t_b = torch.from_numpy(b) # convert numpy array to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t_a)\n",
    "print(t_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_ones = torch.ones(2, 3) # make a matrix of dim 2 x 3 filled with value 1\n",
    "print(t_ones)\n",
    "t_ones.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a tensor of random values\n",
    "t_rand = torch.rand(2,3)\n",
    "print(t_rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulating Datatype and Shape of Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `torch.to()` can be used to change data type of a tensor to a desired type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_a_new = t_a.to(torch.float)\n",
    "print(f\"t_a type = {t_a.dtype}\")\n",
    "print(f\"t_a_new type = {t_a_new.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some operations require that input tensors have a certain dimension (rank) associated with a certain number of elements. We can change the shape of them with the functions `torch.transpose()`, `torch.reshape()` (or `torch.view()`), and `torch.squeeze()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transposing a tensor\n",
    "t = torch.rand(3, 5)\n",
    "t_T = t.transpose(0, 1) # swap dim 0 and 1\n",
    "print(t.shape, \" --> \", t_T.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshaping a tensor\n",
    "t = torch.zeros(30)\n",
    "t_reshape_1 = t.reshape(3, 10)\n",
    "t_reshape_2 = t.view(3,10) # the same\n",
    "print(t.shape, \" --> \", t_reshape_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing unnecessary dimensions (those dimensions of value 1 are not needed)\n",
    "t = torch.zeros(1, 2, 1, 4, 1)\n",
    "t_sqz = t.squeeze(2) # remove 3rd dim of 1\n",
    "print(t.shape, \" --> \", t_sqz.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical Operations on Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = 2*torch.rand(5,2) - 1\n",
    "t2 = torch.normal(mean=0, std=1, size=(5,2))\n",
    "print(t1)\n",
    "print(t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Element Wise Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t3 = t1.multiply(t2)\n",
    "print(t3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Matrix-Matrix Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t4 = torch.matmul(t1, t2.transpose(0, 1))\n",
    "print(t1.shape, \" x \", t2.transpose(0,1).shape, \" = \", t4.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sum, Mean, Standard Deviation, and Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_mean = t1.mean() # across all rows and columns\n",
    "t_mean_row = t1.mean(axis=1)\n",
    "t_mean_col = t1.mean(axis=0)\n",
    "print(t_mean)\n",
    "print(t_mean_row)\n",
    "print(t_mean_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_sum = t1.sum()\n",
    "t_std = t1.std()\n",
    "print(f\"sum = {t_sum}\")\n",
    "print(f\"standard deviation = {t_std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_norm = torch.linalg.norm(t1, ord=2, dim=1) # computer L2 norm of tensor for each roch\n",
    "print(t_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split, Stack and Concatenate Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide a tensor into \"n\" equal chunks\n",
    "t = torch.rand(6)\n",
    "print(f\"t = {t}\")\n",
    "t_splits = torch.chunk(t, chunks=3)\n",
    "[f\"Chunk {i} = {ch}\" for i, ch in enumerate(t_splits)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack or concatenate tensors\n",
    "A = torch.ones(1, 3)\n",
    "B = torch.zeros(4, 3)\n",
    "C = torch.cat([A, B], axis=0)\n",
    "print(f\"A = {A}\")\n",
    "print(f\"B = {B}\")\n",
    "print(f\"C = {C}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Neural Networks with PyTorch\n",
    "\n",
    "In the next section, we will build a simple neural network to classify handwritten digits from the MNIST data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import torchvision\n",
    "\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Loading and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from sklearn.datasets import fetch_openml\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "dataset = fetch_openml(\"mnist_784\", version=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the contents in the dataset\n",
    "print(list(dataset))\n",
    "print(dataset.data.shape) # the images\n",
    "print(dataset.target.shape) # the labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the classes in the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some dataset instances\n",
    "# plot some digits\n",
    "X, y = dataset.data.values, dataset.target.astype(int).values\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=5, sharex=True, sharey=True)\n",
    "ax = ax.flatten() # 1D iterator over array\n",
    "for i in range(10):\n",
    "    img = X[y==i][0].reshape(28,28)\n",
    "    ax[i].imshow(img, cmap=\"Greys\")\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to preprocess the data in some way before input into the neural network. They are sensitive to features with large magnitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "min_max_scale = MinMaxScaler(feature_range=(-1, 1))\n",
    "X_processed = min_max_scale.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert the `sklearn` dataset into one streamlined for `torch`.\n",
    "\n",
    "When model training is running, we will train on train data and evaluate during training on validation data. We will additionally reserve a subset of the data as test data for model evaluation after training is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils import data\n",
    "\n",
    "def to_torch(x):\n",
    "    if torch.is_tensor(x):\n",
    "        return x\n",
    "    return torch.from_numpy(x).to(torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training any Machine Learning algorithm, it is important to have representative instances of the general dataset population. A model will have poor performance if the training instance ratios it sees are not similar to the ratios of the real dataset. This type of equal sampling is called *stratified sampling.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ratio = 0.15\n",
    "val_ratio  = 0.2\n",
    "# split into temp and test\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X_processed, y, test_size=test_ratio, stratify=y)\n",
    "# split into train and validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=val_ratio, stratify=y_temp)\n",
    "\n",
    "# sklearn -> torch\n",
    "X_train_t, y_train_t = to_torch(X_train), to_torch(y_train)\n",
    "X_test_t, y_test_t  = to_torch(X_test), to_torch(y_test)\n",
    "X_val_t, y_val_t = to_torch(X_val), to_torch(y_val)\n",
    "\n",
    "train_dataset = data.TensorDataset(X_train_t, y_train_t)\n",
    "test_dataset  = data.TensorDataset(X_test_t, y_test_t)\n",
    "val_dataset   = data.TensorDataset(X_val_t, y_val_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, the `data.DataLoader` class handles feeding data to the model during training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_size = 100\n",
    "\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=b_size, shuffle=True, drop_last=True, pin_memory=True)\n",
    "val_loader  = data.DataLoader(val_dataset, batch_size=b_size, shuffle=False, drop_last=False)\n",
    "test_loader = data.DataLoader(test_dataset, batch_size=b_size, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing a Neural Network in PyTorch\n",
    "\n",
    "We will implement a simple Multilayer Perceptron to perform a logistic regression task.\n",
    "\n",
    "<img src=\"mlp.png\" alt=\"mlp\" width=\"600\"/>\n",
    "\n",
    "The foundation of building neural networks from PyTorch is done through `torch.nn.Module`. These are the building blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, num_features=784, num_classes=10, hidden_sizes=[50]):\n",
    "        \"\"\" \n",
    "        To add more layers in your network, add additional layer sizes to the \"hidden_sizes\" hyperparam\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        layer_sizes = [num_features] + hidden_sizes + [num_classes]\n",
    "        for i in range(1, len(layer_sizes)):\n",
    "            layers.append(nn.Linear(layer_sizes[i-1], layer_sizes[i]))\n",
    "        layers.append(nn.Sigmoid()) # added for logistic regression task\n",
    "    \n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions\n",
    "Since we are doing a Logistic Regression, the activation function of choice is a sigmoid. There are a myriad of activation functions for deep neural networks, each with its pros and cons. See a summary below or visit this [link](https://pytorch.org/docs/stable/nn.functional.html#non-linear-activation-functions).\n",
    "\n",
    "<img src=\"act_fns.png\" width=\"450\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression()\n",
    "print(log_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network weight initialization is very important as successful training is dependent consistent gradient and variance flow across layers. For more details, see [here](https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.calculate_gain)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(model, factor=1):\n",
    "    for name, param in model.named_parameters():\n",
    "        if name.endswith(\".bias\"):\n",
    "            val = np.random.normal(loc=0.0, scale=0.1)\n",
    "            param.data.fill_(val)\n",
    "        else:\n",
    "            nn.init.xavier_uniform_(param)\n",
    "\n",
    "init_weights(log_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now define the loss function and optimizers for our model. Since we are performing a logistic regression, a suitable loss function is Cross Entropy. For more details on loss functions, see [this link](https://towardsdatascience.com/common-loss-functions-in-machine-learning-46af0ffc4d23).\n",
    "\n",
    "The optimization controls how fast our weights change (i.e. how big of a step is taken when performing gradient descent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_module = nn.CrossEntropyLoss() # compute softmax then cross entropy\n",
    "optim = torch.optim.SGD(log_reg.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader, val_loader, loss_module, num_epochs=50, verbose=False):\n",
    "    # set the model to train mode\n",
    "    model.to(device)\n",
    "\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    pbar = tqdm(range(num_epochs))\n",
    "\n",
    "    for epoch in pbar:\n",
    "        train_loss, train_acc, val_acc = 0.0, 0, 0\n",
    "        train_instances = 0\n",
    "        val_instances = 0\n",
    "\n",
    "        model.train()\n",
    "        for features, labels in train_loader:\n",
    "            # move tensors to GPU (or CPU if no GPU)\n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            predictions = model(features).squeeze(1)\n",
    "            loss = loss_module(predictions, labels.long())\n",
    "            \n",
    "            # MUST ALWAYE DO THIS STEP\n",
    "            optimizer.zero_grad() # clear out residual gradients (very important)\n",
    "\n",
    "            loss.backward() # back propogation\n",
    "            optimizer.step() # run gradient descent\n",
    "\n",
    "            # metrics\n",
    "            train_loss += loss.item()\n",
    "            train_acc += (torch.argmax(predictions, dim=1) == labels).sum().item()\n",
    "\n",
    "            train_instances += features.shape[0]\n",
    "\n",
    "        train_losses.append(train_loss/train_instances)\n",
    "        train_accuracies.append(train_acc/train_instances)\n",
    "\n",
    "        # run validation loop\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for features, labels in val_loader:\n",
    "                # same as train but only forward prop\n",
    "                features = features.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                predictions = model(features).squeeze(1)\n",
    "                val_acc += (torch.argmax(predictions, dim=1) == labels).sum().item()\n",
    "\n",
    "                val_instances += features.shape[0]\n",
    "\n",
    "        val_accuracies.append(val_acc/val_instances)\n",
    "\n",
    "        if verbose:\n",
    "            pbar.set_description(f\"Epoch = {epoch}/{num_epochs} | \"\n",
    "                    f\"Train MSE: {train_loss/train_instances:.4f} | \" \n",
    "                    f\"Train Acc: {train_acc/train_instances*100:.4f}% | \" \n",
    "                    f\"Val Acc: {val_acc/val_instances*100:.4f}%\")\n",
    "\n",
    "    return (train_losses, train_accuracies, val_accuracies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "train_losses, train_accuracies, val_accuracies = train(log_reg, optim, train_loader, val_loader, \n",
    "                                                    loss_module, num_epochs=num_epochs, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the loss and accuracies\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "idx = [i for i in range(num_epochs)]\n",
    "plt.plot(idx, train_losses, label=\"Loss\")\n",
    "plt.title(\"Cross Entropy Loss vs Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid()\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(idx, train_accuracies, label=\"Training Acc\")\n",
    "plt.plot(idx, val_accuracies, label=\"Validation Acc\")\n",
    "plt.title(\"Accuracies vs Epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.grid()\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that after the \"elbow\" of the training graphs, the validation and training accuracies diverge from each other. This gap betweent the two curves is indicative of overfitting. Some things you can do are lower the learning rate, decrease the number of hidden layers, lower the number of epochs, etc.\n",
    "\n",
    "Let's now evaluate the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, test_loader):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    num_examples = 0\n",
    "    acc = 0.0\n",
    "    for features, labels in test_loader:\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        predictions = model(features)\n",
    "        num_examples += features.shape[0]\n",
    "        acc += (torch.argmax(predictions.cpu(), axis=1) == labels.cpu()).sum().item()\n",
    "    acc /= num_examples\n",
    "    print(f\"Test Accuracy = {acc*100:.4f}%.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(log_reg, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test accuracy is similar to the train and validation accuracies. Play around with the model to try and improve your score!\n",
    "\n",
    "Some things you can try are:\n",
    "1. Data preprocessing - transform the training inputs more to lower the bias of the model.\n",
    "2. Hyperparameter tuning - change the number of layers, the hidden dimension size, the learning rate, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and Reloading Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"simple_log_reg.pt\"\n",
    "torch.save(log_reg, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the model\n",
    "log_reg_reload = torch.load(model_path)\n",
    "# verify that the model is the same\n",
    "print(log_reg_reload.eval())\n",
    "# now verfiy the weights are the same by evaluating it on test set\n",
    "evaluate(log_reg_reload, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving only the learned parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"simple_log_reg_state.pt\"\n",
    "torch.save(log_reg.state_dict(), path)\n",
    "\n",
    "# reload saved parameters to reconstruct model\n",
    "log_reg_reload = LogisticRegression()\n",
    "log_reg_reload.load_state_dict(torch.load(path)) \n",
    "\n",
    "# now verfiy the weights are the same by evaluating it on test set\n",
    "evaluate(log_reg_reload, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pytorch-intro')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "028fef89f48f7384f9ee9af194638d3635a9d76db122a5b079c2e8f119405309"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
